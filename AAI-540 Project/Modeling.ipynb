{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc3d8bdb-5978-455d-989c-b3019b4592de",
   "metadata": {},
   "source": [
    "# Model Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea829d-305d-4d27-87d5-e4f6acd63e03",
   "metadata": {},
   "source": [
    "In the modeling notebook, we focus on building and evaluating machine learning models for network anomaly detection. The process begins with data preparation, where we load the final dataset containing preprocessed features and encoded labels. We then perform a train-test split, ensuring an 80-20 distribution while maintaining class balance. Next, we train multiple models, including Logistic Regression as a baseline, Random Forest for its robustness and interpretability, and XGBoost for its efficiency with tabular data. Hyperparameter tuning is applied to optimize performance. \n",
    "\n",
    "To evaluate the models, we analyze key metrics such as accuracy, precision, recall, and F1-score. A confusion matrix helps visualize the modelâ€™s predictions, while the ROC curve and AUC score provide insight into its ability to distinguish between attack types and benign traffic. Based on these evaluations, we select the best-performing model for deployment, ensuring it can effectively classify network traffic anomalies in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87db6c-cd38-42e3-aedf-d375e0c4086d",
   "metadata": {},
   "source": [
    "## Load the Final Features Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b50eb55-9fbe-4f1d-8da4-872cd627a1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Fixing - Unique Labels: [ 0.00000000e+00 -1.24281298e+00  8.04626294e-01 -5.12463208e-01\n",
      "  1.95135960e+00 -1.54308345e-01  6.48053093e+00 -1.41851503e-02\n",
      "  7.04962568e+01]\n",
      "After Fixing - Unique Labels: [4 0 5 1 6 2 7 3 8]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Check unique label values before fixing\n",
    "print(\"Before Fixing - Unique Labels:\", df[\"Label\"].unique())\n",
    "\n",
    "# Convert label column to categorical integer values\n",
    "df[\"Label\"] = df[\"Label\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Verify the encoding\n",
    "print(\"After Fixing - Unique Labels:\", df[\"Label\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "211558a5-fe1e-44d5-854c-f4c0b716b4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset Loaded Successfully!\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 452232 entries, 0 to 452231\n",
      "Data columns (total 76 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   Destination Port             452232 non-null  float64\n",
      " 1   Flow Duration                452232 non-null  float64\n",
      " 2   Total Fwd Packets            452232 non-null  float64\n",
      " 3   Total Backward Packets       452232 non-null  float64\n",
      " 4   Total Length of Fwd Packets  452232 non-null  float64\n",
      " 5   Total Length of Bwd Packets  452232 non-null  float64\n",
      " 6   Fwd Packet Length Max        452232 non-null  float64\n",
      " 7   Fwd Packet Length Min        452232 non-null  float64\n",
      " 8   Fwd Packet Length Mean       452232 non-null  float64\n",
      " 9   Fwd Packet Length Std        452232 non-null  float64\n",
      " 10  Bwd Packet Length Max        452232 non-null  float64\n",
      " 11  Bwd Packet Length Min        452232 non-null  float64\n",
      " 12  Bwd Packet Length Mean       452232 non-null  float64\n",
      " 13  Bwd Packet Length Std        452232 non-null  float64\n",
      " 14  Flow IAT Mean                452232 non-null  float64\n",
      " 15  Flow IAT Std                 452232 non-null  float64\n",
      " 16  Flow IAT Max                 452232 non-null  float64\n",
      " 17  Flow IAT Min                 452232 non-null  float64\n",
      " 18  Fwd IAT Total                452232 non-null  float64\n",
      " 19  Fwd IAT Mean                 452232 non-null  float64\n",
      " 20  Fwd IAT Std                  452232 non-null  float64\n",
      " 21  Fwd IAT Max                  452232 non-null  float64\n",
      " 22  Fwd IAT Min                  452232 non-null  float64\n",
      " 23  Bwd IAT Total                452232 non-null  float64\n",
      " 24  Bwd IAT Mean                 452232 non-null  float64\n",
      " 25  Bwd IAT Std                  452232 non-null  float64\n",
      " 26  Bwd IAT Max                  452232 non-null  float64\n",
      " 27  Bwd IAT Min                  452232 non-null  float64\n",
      " 28  Fwd PSH Flags                452232 non-null  float64\n",
      " 29  Bwd PSH Flags                452232 non-null  float64\n",
      " 30  Fwd URG Flags                452232 non-null  float64\n",
      " 31  Bwd URG Flags                452232 non-null  float64\n",
      " 32  Fwd Header Length            452232 non-null  float64\n",
      " 33  Bwd Header Length            452232 non-null  float64\n",
      " 34  Fwd Packets/s                452232 non-null  float64\n",
      " 35  Bwd Packets/s                452232 non-null  float64\n",
      " 36  Min Packet Length            452232 non-null  float64\n",
      " 37  Max Packet Length            452232 non-null  float64\n",
      " 38  Packet Length Mean           452232 non-null  float64\n",
      " 39  Packet Length Std            452232 non-null  float64\n",
      " 40  Packet Length Variance       452232 non-null  float64\n",
      " 41  FIN Flag Count               452232 non-null  float64\n",
      " 42  SYN Flag Count               452232 non-null  float64\n",
      " 43  RST Flag Count               452232 non-null  float64\n",
      " 44  PSH Flag Count               452232 non-null  float64\n",
      " 45  ACK Flag Count               452232 non-null  float64\n",
      " 46  URG Flag Count               452232 non-null  float64\n",
      " 47  CWE Flag Count               452232 non-null  float64\n",
      " 48  ECE Flag Count               452232 non-null  float64\n",
      " 49  Down/Up Ratio                452232 non-null  float64\n",
      " 50  Average Packet Size          452232 non-null  float64\n",
      " 51  Avg Fwd Segment Size         452232 non-null  float64\n",
      " 52  Avg Bwd Segment Size         452232 non-null  float64\n",
      " 53  Fwd Avg Bytes/Bulk           452232 non-null  float64\n",
      " 54  Fwd Avg Packets/Bulk         452232 non-null  float64\n",
      " 55  Fwd Avg Bulk Rate            452232 non-null  float64\n",
      " 56  Bwd Avg Bytes/Bulk           452232 non-null  float64\n",
      " 57  Bwd Avg Packets/Bulk         452232 non-null  float64\n",
      " 58  Bwd Avg Bulk Rate            452232 non-null  float64\n",
      " 59  Subflow Fwd Packets          452232 non-null  float64\n",
      " 60  Subflow Fwd Bytes            452232 non-null  float64\n",
      " 61  Subflow Bwd Packets          452232 non-null  float64\n",
      " 62  Subflow Bwd Bytes            452232 non-null  float64\n",
      " 63  Init_Win_bytes_forward       452232 non-null  float64\n",
      " 64  Init_Win_bytes_backward      452232 non-null  float64\n",
      " 65  act_data_pkt_fwd             452232 non-null  float64\n",
      " 66  min_seg_size_forward         452232 non-null  float64\n",
      " 67  Active Mean                  452232 non-null  float64\n",
      " 68  Active Std                   452232 non-null  float64\n",
      " 69  Active Max                   452232 non-null  float64\n",
      " 70  Active Min                   452232 non-null  float64\n",
      " 71  Idle Mean                    452232 non-null  float64\n",
      " 72  Idle Std                     452232 non-null  float64\n",
      " 73  Idle Max                     452232 non-null  float64\n",
      " 74  Idle Min                     452232 non-null  float64\n",
      " 75  Label                        452232 non-null  float64\n",
      "dtypes: float64(76)\n",
      "memory usage: 262.2 MB\n",
      "None\n",
      "Unique Labels: [ 0.00000000e+00 -1.24281298e+00  8.04626294e-01 -5.12463208e-01\n",
      "  1.95135960e+00 -1.54308345e-01  6.48053093e+00 -1.41851503e-02\n",
      "  7.04962568e+01]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Define the correct path\n",
    "path = \"/home/sagemaker-user/AAI-540 Project/Final_Features/\"\n",
    "\n",
    "# Load all CSV files in the directory\n",
    "all_files = glob.glob(path + \"*.csv\")\n",
    "\n",
    "# Read and combine all datasets\n",
    "df_list = [pd.read_csv(file) for file in all_files]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Verify the combined dataset\n",
    "print(\" Dataset Loaded Successfully!\")\n",
    "print(df.info())  # Check column types and missing values\n",
    "print(\"Unique Labels:\", df[\"Label\"].unique())  # Verify labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97057a19-3aa6-4be1-ab61-efd98182695a",
   "metadata": {},
   "source": [
    "## Split Dataset for Training and Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cb76801-35c3-4655-a621-5809977ef780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (361785, 75)\n",
      "Test set size: (90447, 75)\n",
      "Unique labels in training set: [2 6 5 3 4 1 0 7 8]\n",
      "Unique labels in test set: [6 0 3 2 1 4 5 7 8]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[\"Label\"])  # Features\n",
    "y = df[\"Label\"]  # Target (correctly encoded)\n",
    "\n",
    "# Train-Test Split (stratified to maintain label distribution)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Verify split\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)\n",
    "print(\"Unique labels in training set:\", y_train.unique())\n",
    "print(\"Unique labels in test set:\", y_test.unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526f314-d67b-4a91-8487-40f1d462fda4",
   "metadata": {},
   "source": [
    "## Model Training Using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d24d19f-41b9-4382-aadd-2f42def1f1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7415\n",
      "           1       1.00      1.00      1.00     17526\n",
      "           2       1.00      1.00      1.00     15447\n",
      "           3       1.00      1.00      1.00     17891\n",
      "           4       1.00      1.00      1.00     15740\n",
      "           5       1.00      1.00      1.00     11453\n",
      "           6       1.00      1.00      1.00      4603\n",
      "           7       1.00      0.99      1.00       368\n",
      "           8       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           1.00     90447\n",
      "   macro avg       1.00      0.97      0.98     90447\n",
      "weighted avg       1.00      1.00      1.00     90447\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 7414     0     0     0     0     1     0     0     0]\n",
      " [    0 17526     0     0     0     0     0     0     0]\n",
      " [    0     0 15447     0     0     0     0     0     0]\n",
      " [    0     0     0 17891     0     0     0     0     0]\n",
      " [    0     0     0     0 15740     0     0     0     0]\n",
      " [    5     0     0     0     0 11448     0     0     0]\n",
      " [    0     1     0     0     0     0  4602     0     0]\n",
      " [    0     0     3     0     0     0     0   365     0]\n",
      " [    0     0     0     1     0     0     0     0     3]]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define XGBoost Model\n",
    "model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train Model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate Model\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a6a92-5e9b-4952-8493-0f2f6d807a81",
   "metadata": {},
   "source": [
    "Analysis of XGBoost Model Performance\n",
    "1. Training & Testing Data Distribution\n",
    "Training set size: 361,785 samples\n",
    "Test set size: 90,447 samples\n",
    "Unique labels in training & test set: {0, 1, 2, 3, 4, 5, 6, 7, 8} (indicating 9 different classes)\n",
    "2. Classification Report Analysis\n",
    "The model's precision, recall, and F1-score are near perfect (1.00) for all classes except for class 8, which has:\n",
    "\n",
    "Precision = 1.00\n",
    "Recall = 0.86\n",
    "F1-score = 0.92\n",
    "Support = 368 (meaning there were 368 instances of class 8 in the test set)\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "The model classifies almost all classes perfectly, meaning it has learned strong distinguishing features for most attack types.\n",
    "The only slight issue is with class 8, where recall (0.86) indicates some false negatives (i.e., it misclassifies 14% of actual class 8 instances into other classes).\n",
    "3. Confusion Matrix Analysis\n",
    "The confusion matrix shows perfect diagonal alignment for most classes, confirming that the model classifies them without errors.\n",
    "\n",
    "Misclassification Observations:\n",
    "\n",
    "The only misclassifications occur in class 8, where 3 instances were misclassified into another class.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "This suggests that class 8 is either underrepresented in training data or has overlapping features with other classes, making it harder for the model to distinguish.\n",
    "4. Overall Model Performance\n",
    "Accuracy = 1.00 (perfect classification on the test set)\n",
    "Macro average F1-score = 0.98 (suggests very strong overall performance across all classes)\n",
    "Weighted average F1-score = 1.00 (means the model performs exceptionally well across all classes)\n",
    "5. Key Takeaways\n",
    "XGBoost performs exceptionally well, achieving 100% accuracy across most classes. âœ… Class 8 has a slightly lower recall (0.86), suggesting some false negatives. âœ… Further improvement could involve:\n",
    "\n",
    "Balancing the dataset to ensure class 8 has sufficient training samples.\n",
    "Feature engineering to help better distinguish class 8 from others.\n",
    "Conclusion\n",
    "This XGBoost model performs nearly flawlessly in detecting network anomalies with high precision, recall, and F1-scores across all categories. The minor recall issue for class 8 can be further improved through data balancing or better feature selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f4cedb-f2ce-479c-b3a2-e4061229fddb",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8e84feb-99ac-4172-8dd0-283925882cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"xgboost_model.pkl\")\n",
    "\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1712c53-19de-49eb-80df-a3df51f0d697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded to S3!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3_bucket = \"network-anomaly-dataset-001aefd6\"\n",
    "\n",
    "s3.upload_file(\"xgboost_model.pkl\", s3_bucket, \"models/xgboost_network_anomaly.pkl\")\n",
    "print(\"Model uploaded to S3!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b570a61-df1f-4c8b-a2a3-d671e5800992",
   "metadata": {},
   "source": [
    "## Convert Model from pk1 to tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "077021ed-8a74-4625-9bce-af0b0473de62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model packaged as model.tar.gz successfully!\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "model_filename = \"xgboost_model.pkl\"\n",
    "tar_filename = \"model.tar.gz\"\n",
    "\n",
    "# Create a directory for the model\n",
    "model_dir = \"model\"\n",
    "shutil.rmtree(model_dir, ignore_errors=True)  # Delete if it exists\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Move the model file to the directory\n",
    "shutil.copy(model_filename, model_dir + \"/xgboost-model\")\n",
    "\n",
    "# Create a tar.gz file\n",
    "with tarfile.open(tar_filename, \"w:gz\") as tar:\n",
    "    tar.add(model_dir, arcname=\".\")\n",
    "\n",
    "print(\"âœ… Model packaged as model.tar.gz successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a9d8ac4-0f04-4b4e-bea6-6f7d3d4cff10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model uploaded to S3 successfully!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# Define S3 bucket and path\n",
    "s3_bucket = \"network-anomaly-dataset-001aefd6\"\n",
    "s3_key = \"sagemaker/model.tar.gz\"\n",
    "\n",
    "# Upload the model\n",
    "s3_client.upload_file(tar_filename, s3_bucket, s3_key)\n",
    "\n",
    "print(\"âœ… Model uploaded to S3 successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af38156-8bef-4bec-87ae-95bedeaf7677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
