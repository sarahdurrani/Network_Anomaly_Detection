{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f224bde-6c97-4e3b-8598-7653d5e26286",
   "metadata": {},
   "source": [
    "# Data Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d34ab25-0484-42ca-a11f-c671f5ad36ff",
   "metadata": {},
   "source": [
    "## Load Dataset from S3 Bucket and Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef54e29d-885f-4f7b-87d7-728ae6d81a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Monday-WorkingHours.pcap_ISCX.csv from S3...\n",
      "Monday-WorkingHours.pcap_ISCX.csv loaded successfully!\n",
      "Loading Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv from S3...\n",
      "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv loaded successfully!\n",
      "Loading Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv from S3...\n",
      "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv loaded successfully!\n",
      "Loading Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv from S3...\n",
      "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv loaded successfully!\n",
      "Loading Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv from S3...\n",
      "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv loaded successfully!\n",
      "\n",
      "Monday-WorkingHours.pcap_ISCX.csv - Unique Labels After Cleaning:\n",
      "['BENIGN']\n",
      "\n",
      "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv - Unique Labels After Cleaning:\n",
      "['BENIGN' 'DDoS']\n",
      "\n",
      "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv - Unique Labels After Cleaning:\n",
      "['BENIGN' 'PortScan']\n",
      "\n",
      "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv - Unique Labels After Cleaning:\n",
      "['BENIGN' 'WEB ATTACK']\n",
      "\n",
      "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv - Unique Labels After Cleaning:\n",
      "['BENIGN' 'Infiltration']\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "s3_bucket_name = \"network-anomaly-dataset-001aefd6\"  # Replace with your actual bucket name\n",
    "file_names = [\n",
    "    \"Monday-WorkingHours.pcap_ISCX.csv\",\n",
    "    \"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\",\n",
    "    \"Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\",\n",
    "    \"Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\",\n",
    "    \"Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\"\n",
    "]\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "dfs = {}\n",
    "\n",
    "for file_name in file_names:\n",
    "    print(f\"Loading {file_name} from S3...\")\n",
    "\n",
    "    obj = s3.get_object(Bucket=s3_bucket_name, Key=file_name)\n",
    "\n",
    "    # Try different encodings and delimiters\n",
    "    try:\n",
    "        df = pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')), sep=',', engine='python')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(StringIO(obj['Body'].read().decode('ISO-8859-1')), sep=',', engine='python')\n",
    "\n",
    "    # Strip spaces from column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # Check if \"Label\" column exists\n",
    "    if \"Label\" not in df.columns:\n",
    "        print(f\"⚠ WARNING: {file_name} is missing the 'Label' column!\")\n",
    "\n",
    "    # Store cleaned dataframe\n",
    "    dfs[file_name] = df\n",
    "    print(f\"{file_name} loaded successfully!\")\n",
    "\n",
    "# ✅ Fill missing labels safely\n",
    "for name, df in dfs.items():\n",
    "    if \"Label\" in df.columns:\n",
    "        df.loc[:, \"Label\"] = df[\"Label\"].fillna(\"BENIGN\")  # Safe method\n",
    "\n",
    "# ✅ Fix Encoding Errors (e.g., \"Web Attack � Brute Force\")\n",
    "label_cleaning = {\n",
    "    \"Web Attack � Brute Force\": \"WEB ATTACK\",\n",
    "    \"Web Attack � XSS\": \"WEB ATTACK\"\n",
    "}\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    if \"Label\" in df.columns:\n",
    "        df.loc[:, \"Label\"] = df[\"Label\"].replace(label_cleaning)  # Safe replacement\n",
    "\n",
    "# ✅ Print unique labels after cleaning\n",
    "for name, df in dfs.items():\n",
    "    print(f\"\\n{name} - Unique Labels After Cleaning:\")\n",
    "    print(df[\"Label\"].unique() if \"Label\" in df.columns else \"WARNING: Label column missing!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c32cf-f55c-483b-81c8-0298c37b1956",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a75c733-2309-49da-8388-2a39dc1f9049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Monday-WorkingHours.pcap_ISCX.csv - Encoded Label Values: [0]\n",
      "\n",
      "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv - Encoded Label Values: [0 1]\n",
      "\n",
      "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv - Encoded Label Values: [0 2]\n",
      "\n",
      "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv - Encoded Label Values: [0 3]\n",
      "\n",
      "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv - Encoded Label Values: [0 4]\n"
     ]
    }
   ],
   "source": [
    "# Define Label Encoding Mapping\n",
    "label_mapping = {\n",
    "    \"BENIGN\": 0,\n",
    "    \"DDoS\": 1,\n",
    "    \"PortScan\": 2,\n",
    "    \"WEB ATTACK\": 3,\n",
    "    \"Infiltration\": 4\n",
    "}\n",
    "\n",
    "# Apply Encoding\n",
    "for name, df in dfs.items():\n",
    "    if \"Label\" in df.columns:\n",
    "        df[\"Label\"] = df[\"Label\"].map(label_mapping)\n",
    "\n",
    "# Verify Encoding\n",
    "for name, df in dfs.items():\n",
    "    print(f\"\\n{name} - Encoded Label Values: {df['Label'].unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94c3afab-4d28-4bbf-9c55-337dcf02f5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Monday-WorkingHours.pcap_ISCX.csv - Unique Encoded Label Values: [0]\n",
      "Missing values in Label column: 0\n",
      "\n",
      "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv - Unique Encoded Label Values: [0 1]\n",
      "Missing values in Label column: 0\n",
      "\n",
      "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv - Unique Encoded Label Values: [0 2]\n",
      "Missing values in Label column: 0\n",
      "\n",
      "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv - Unique Encoded Label Values: [0 3]\n",
      "Missing values in Label column: 0\n",
      "\n",
      "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv - Unique Encoded Label Values: [0 4]\n",
      "Missing values in Label column: 0\n"
     ]
    }
   ],
   "source": [
    "for name, df in dfs.items():\n",
    "    print(f\"\\n{name} - Unique Encoded Label Values: {df['Label'].unique()}\")\n",
    "    print(f\"Missing values in Label column: {df['Label'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9f96f2-6935-41a5-8467-87668a10b877",
   "metadata": {},
   "source": [
    "## Save Processed Data for Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5f53e98-878f-483e-b75a-10f3ed7bb1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: processed_Monday-WorkingHours.pcap_ISCX.csv to S3.\n",
      "✅ Uploaded: processed_Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv to S3.\n",
      "✅ Uploaded: processed_Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv to S3.\n",
      "✅ Uploaded: processed_Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv to S3.\n",
      "✅ Uploaded: processed_Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv to S3.\n"
     ]
    }
   ],
   "source": [
    "# Save Processed Data Locally & Upload to S3\n",
    "for name, df in dfs.items():\n",
    "    processed_file = f\"processed_{name}\"\n",
    "    df.to_csv(processed_file, index=False)  # Save locally\n",
    "    \n",
    "    # Upload to S3\n",
    "    s3.upload_file(processed_file, s3_bucket_name, f\"processed/{processed_file}\")\n",
    "    print(f\"✅ Uploaded: {processed_file} to S3.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31dd891-5388-4f0e-ad02-88fd39cfc9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
