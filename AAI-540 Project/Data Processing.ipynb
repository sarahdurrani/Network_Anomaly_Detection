{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f224bde-6c97-4e3b-8598-7653d5e26286",
   "metadata": {},
   "source": [
    "# Data Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d34ab25-0484-42ca-a11f-c671f5ad36ff",
   "metadata": {},
   "source": [
    "## Load Dataset from S3 Bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef54e29d-885f-4f7b-87d7-728ae6d81a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Monday-WorkingHours.pcap_ISCX.csv from S3...\n",
      "Monday-WorkingHours.pcap_ISCX.csv loaded successfully!\n",
      "Loading Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv from S3...\n",
      "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv loaded successfully!\n",
      "Loading Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv from S3...\n",
      "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv loaded successfully!\n",
      "Loading Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv from S3...\n",
      "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv loaded successfully!\n",
      "Loading Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv from S3...\n",
      "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv loaded successfully!\n",
      "\n",
      "Monday-WorkingHours.pcap_ISCX.csv - Unique Labels After Reloading:\n",
      "['BENIGN' None]\n",
      "\n",
      "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv - Unique Labels After Reloading:\n",
      "['BENIGN' 'DDoS' None]\n",
      "\n",
      "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv - Unique Labels After Reloading:\n",
      "['BENIGN' 'PortScan' None]\n",
      "\n",
      "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv - Unique Labels After Reloading:\n",
      "['BENIGN' 'Web Attack � Brute Force' 'Web Attack � XSS' None]\n",
      "\n",
      "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv - Unique Labels After Reloading:\n",
      "['BENIGN' 'Infiltration' None]\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "s3_bucket_name = \"network-anomaly-dataset-001aefd6\"  # Replace with your actual bucket name\n",
    "file_names = [\n",
    "    \"Monday-WorkingHours.pcap_ISCX.csv\",\n",
    "    \"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\",\n",
    "    \"Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\",\n",
    "    \"Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\",\n",
    "    \"Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\"\n",
    "]\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "dfs = {}\n",
    "\n",
    "for file_name in file_names:\n",
    "    print(f\"Loading {file_name} from S3...\")\n",
    "\n",
    "    obj = s3.get_object(Bucket=s3_bucket_name, Key=file_name)\n",
    "\n",
    "    # Try different encodings and delimiters\n",
    "    try:\n",
    "        df = pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')), sep=',', engine='python')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(StringIO(obj['Body'].read().decode('ISO-8859-1')), sep=',', engine='python')\n",
    "\n",
    "    # Strip spaces from column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # Store cleaned dataframe\n",
    "    dfs[file_name] = df\n",
    "    print(f\"{file_name} loaded successfully!\")\n",
    "\n",
    "# Check if \"Label\" column now contains valid values\n",
    "for name, df in dfs.items():\n",
    "    print(f\"\\n{name} - Unique Labels After Reloading:\")\n",
    "    print(df[\"Label\"].unique() if \"Label\" in df.columns else \"WARNING: Label column missing!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9621f3-a856-4a06-9263-230a630623df",
   "metadata": {},
   "source": [
    "This will load each csv file from S3 into Pandas Dataframes and store them in a dictionary for easy access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58997059-c03a-4115-b0af-ec64dd89f77b",
   "metadata": {},
   "source": [
    "## Data Clean-Up and Label Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96f4985e-8ca8-4b2f-93fb-a088f329cd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Monday-WorkingHours.pcap_ISCX.csv - Unique Labels After Cleaning:\n",
      "['BENIGN']\n",
      "\n",
      "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv - Unique Labels After Cleaning:\n",
      "['BENIGN' 'DDOS']\n",
      "\n",
      "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv - Unique Labels After Cleaning:\n",
      "['BENIGN' 'PORTSCAN']\n",
      "\n",
      "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv - Unique Labels After Cleaning:\n",
      "['BENIGN' 'WEB ATTACK']\n",
      "\n",
      "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv - Unique Labels After Cleaning:\n",
      "['BENIGN' 'INFILTRATION']\n"
     ]
    }
   ],
   "source": [
    "# Ensure we are working with a copy of the DataFrame\n",
    "for name in dfs:\n",
    "    dfs[name] = dfs[name].copy()  # Prevent unintended slicing issues\n",
    "\n",
    "# Replace missing labels (None or NaN) with \"BENIGN\"\n",
    "for name in dfs:\n",
    "    dfs[name].loc[dfs[name][\"Label\"].isna(), \"Label\"] = \"BENIGN\"\n",
    "\n",
    "# Normalize label formatting: Convert to uppercase, remove spaces, and standardize names\n",
    "for name in dfs:\n",
    "    dfs[name][\"Label\"] = dfs[name][\"Label\"].astype(str)  # Ensure it's a string\n",
    "    dfs[name][\"Label\"] = dfs[name][\"Label\"].str.strip().str.upper()  # Remove spaces and standardize case\n",
    "    dfs[name][\"Label\"] = dfs[name][\"Label\"].replace({\n",
    "        \"WEB ATTACK � BRUTE FORCE\": \"WEB ATTACK\",\n",
    "        \"WEB ATTACK � XSS\": \"WEB ATTACK\"\n",
    "    })\n",
    "\n",
    "# Verify the label normalization worked\n",
    "for name, df in dfs.items():\n",
    "    print(f\"\\n{name} - Unique Labels After Cleaning:\")\n",
    "    print(df[\"Label\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a33bef-f1af-4fc2-b48f-365b0f6dfa60",
   "metadata": {},
   "source": [
    "### Enconding Labels into Numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93529a37-7cf1-4c42-b5a1-be2c38a50e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Monday-WorkingHours.pcap_ISCX.csv - Unique Encoded Label Values: [0]\n",
      "\n",
      "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv - Unique Encoded Label Values: [0 1]\n",
      "\n",
      "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv - Unique Encoded Label Values: [0 2]\n",
      "\n",
      "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv - Unique Encoded Label Values: [0 3]\n",
      "\n",
      "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv - Unique Encoded Label Values: [0 4]\n"
     ]
    }
   ],
   "source": [
    "# Define label encoding mapping\n",
    "label_mapping = {\n",
    "    \"BENIGN\": 0,\n",
    "    \"DDOS\": 1,\n",
    "    \"PORTSCAN\": 2,\n",
    "    \"WEB ATTACK\": 3,\n",
    "    \"INFILTRATION\": 4\n",
    "}\n",
    "\n",
    "# Apply encoding to the \"Label\" column\n",
    "for name in dfs:\n",
    "    dfs[name][\"Label\"] = dfs[name][\"Label\"].map(label_mapping)\n",
    "\n",
    "# Verify encoding\n",
    "for name, df in dfs.items():\n",
    "    print(f\"\\n{name} - Unique Encoded Label Values: {df['Label'].unique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9f96f2-6935-41a5-8467-87668a10b877",
   "metadata": {},
   "source": [
    "## Save Processed Data for Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5f53e98-878f-483e-b75a-10f3ed7bb1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded processed file: processed_Monday-WorkingHours.pcap_ISCX.csv to S3.\n",
      "Uploaded processed file: processed_Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv to S3.\n",
      "Uploaded processed file: processed_Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv to S3.\n",
      "Uploaded processed file: processed_Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv to S3.\n",
      "Uploaded processed file: processed_Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv to S3.\n"
     ]
    }
   ],
   "source": [
    "for name, df in dfs.items():\n",
    "    processed_file = f\"processed_{name}\"\n",
    "    df.to_csv(processed_file, index=False)\n",
    "    s3.upload_file(processed_file, s3_bucket_name, f\"processed/{processed_file}\")\n",
    "    print(f\"Uploaded processed file: {processed_file} to S3.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
