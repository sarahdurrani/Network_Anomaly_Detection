{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd017f7c-29cf-482d-b95f-408a7e0d1d6f",
   "metadata": {},
   "source": [
    "# Feature Enigneering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023effc5-eb9d-4234-a1af-8fc9fa60cb8a",
   "metadata": {},
   "source": [
    "Feature engineering is a crucial step in preparing data for model training, as it directly impacts the performance and interpretability of the network anomaly detection system. First, we load the processed datasets to ensure that all preprocessing steps, such as label encoding and standardization, have been correctly applied. Handling missing values is essential because gaps in the data can distort model predictions; we address this by using median or mode imputation to maintain consistency while preserving data distribution. Next, we perform feature selection to remove redundant, highly correlated, or low-variance features that add noise and increase computational complexity, thereby improving model efficiency and generalization. Feature transformation, including scaling numerical features, ensures that modelsâ€”especially those relying on distance-based calculationsâ€”are not biased by large numerical ranges. Additionally, encoding categorical variables into numerical representations enables machine learning algorithms to process them effectively. Finally, we save and upload the transformed dataset to AWS S3, ensuring version control, accessibility, and reproducibility for model training. By carefully engineering features, we enhance model accuracy, prevent overfitting, and optimize training time, ultimately leading to a more robust and reliable network anomaly detection system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d76b963d-fe1c-49ed-b37a-13fe6fa9baab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded processed_Monday-WorkingHours.pcap_ISCX.csv - Shape: (78701, 79)\n",
      " Loaded processed_Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv - Shape: (94342, 79)\n",
      " Loaded processed_Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv - Shape: (110641, 79)\n",
      " Loaded processed_Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv - Shape: (79071, 79)\n",
      " Loaded processed_Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv - Shape: (89472, 79)\n",
      "Dropped unnecessary columns.\n",
      "Missing values handled.\n",
      "Features scaled.\n",
      " Saved: Final_Features/final_Monday-WorkingHours.pcap_ISCX.csv\n",
      " Uploaded to S3: final_Monday-WorkingHours.pcap_ISCX.csv\n",
      " Saved: Final_Features/final_Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      " Uploaded to S3: final_Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      " Saved: Final_Features/final_Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      " Uploaded to S3: final_Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      " Saved: Final_Features/final_Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      " Uploaded to S3: final_Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      " Saved: Final_Features/final_Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      " Uploaded to S3: final_Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "Feature Engineering Completed!\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering Notebook (Feature_Engineering.ipynb)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3  # AWS S3 for data handling\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ðŸ”¹ Load Processed Data from S3 (or local directory)\n",
    "s3_bucket_name = \"network-anomaly-dataset-001aefd6\"\n",
    "processed_data_prefix = \"processed/\"\n",
    "\n",
    "# List of processed files\n",
    "selected_files = [\n",
    "    \"processed_Monday-WorkingHours.pcap_ISCX.csv\",\n",
    "    \"processed_Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\",\n",
    "    \"processed_Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\",\n",
    "    \"processed_Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\",\n",
    "    \"processed_Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\"\n",
    "]\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "for file_name in selected_files:\n",
    "    file_path = processed_data_prefix + file_name\n",
    "    obj = s3.get_object(Bucket=s3_bucket_name, Key=file_path)\n",
    "    df = pd.read_csv(obj[\"Body\"])\n",
    "    dfs[file_name] = df\n",
    "    print(f\" Loaded {file_name} - Shape: {df.shape}\")\n",
    "\n",
    "# ðŸ”¹ Feature Selection - Drop Unnecessary Columns\n",
    "drop_columns = [\"Flow Bytes/s\", \"Flow Packets/s\", \"Fwd Header Length.1\"]  \n",
    "\n",
    "for name, df in dfs.items():\n",
    "    df.drop(columns=drop_columns, inplace=True, errors=\"ignore\")\n",
    "    dfs[name] = df\n",
    "\n",
    "print(\"Dropped unnecessary columns.\")\n",
    "\n",
    "# ðŸ”¹ Handle Missing Values\n",
    "for name, df in dfs.items():\n",
    "    df.fillna(df.median(), inplace=True)  # Fill NaN values with median\n",
    "    dfs[name] = df\n",
    "\n",
    "print(\"Missing values handled.\")\n",
    "\n",
    "# ðŸ”¹ Feature Scaling - Normalize Numeric Data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "    dfs[name] = df\n",
    "\n",
    "print(\"Features scaled.\")\n",
    "\n",
    "# ðŸ”¹ Save Final Features Dataset Locally & Upload to S3\n",
    "output_path = \"Final_Features/\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    output_file = output_path + name.replace(\"processed_\", \"final_\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\" Saved: {output_file}\")\n",
    "\n",
    "    # Upload to S3\n",
    "    s3.upload_file(output_file, s3_bucket_name, \"Final_Features/\" + name.replace(\"processed_\", \"final_\"))\n",
    "    print(f\" Uploaded to S3: {name.replace('processed_', 'final_')}\")\n",
    "\n",
    "print(\"Feature Engineering Completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6a1b94-b828-4994-99fa-3354552685f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
